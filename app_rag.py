# app_rag.py
import os, gradio as gr, fitz
from openai import OpenAI
import pytesseract
from PIL import Image

# ---- Ayarlar ve Yollar ----
# Tesseract OCR Yolu (Docker'da 'tesseract', Windows'ta tam yol)
tess_env = os.getenv("TESSERACT_CMD")
if tess_env:
    pytesseract.pytesseract.tesseract_cmd = tess_env
else:
    # Windows varsayÄ±lanÄ± (Fallback)
    pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

# ---- LLM: yerel llama-server (OpenAI uyumlu) ----
# Docker iÃ§inden "llm-server", host Ã¼zerinden "127.0.0.1"
base_url = os.getenv("LLM_API_URL", "http://127.0.0.1:8080/v1")
client = OpenAI(api_key="local", base_url=base_url)

# ---- VektÃ¶r veritabanÄ± & embedding ----
import chromadb
from chromadb.config import Settings
from fastembed import TextEmbedding

# TÃ¼rkÃ§e iÃ§in Ã§ok dilli model (daha iyi eÅŸleÅŸme):
EMBED_MODEL = "BAAI/bge-small-en-v1.5"   # alternatif: "BAAI/bge-small-en-v1.5" (daha hafif)
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 150
TOP_K = 6
MAX_ANSWER_TOKENS = 700
PERSIST_DIR = "./rag_store"
COLLECTION_NAME = "pdf_chunks"

embedder = TextEmbedding(model_name=EMBED_MODEL)

db = chromadb.PersistentClient(path=PERSIST_DIR, settings=Settings(allow_reset=True))
try:
    col = db.get_collection(COLLECTION_NAME)
except:
    col = db.create_collection(COLLECTION_NAME, metadata={"hnsw:space": "cosine"})

def ocr_page(page_pixmap):
    """PyMuPDF pixmap'ini PIL Image'a Ã§evirip OCR yapar."""
    try:
        # Pixmap'ten PIL Image oluÅŸtur
        img = Image.frombytes("RGB", [page_pixmap.width, page_pixmap.height], page_pixmap.samples)
        # TÃ¼rkÃ§e OCR (lang='tur')
        text = pytesseract.image_to_string(img, lang='tur')
        return text
    except Exception as e:
        print(f"OCR HatasÄ±: {e}")
        return ""

def _read_pdf(path):
    doc = fitz.open(path)
    parts = []
    
    print(f"PDF Analiz ediliyor: {path}")
    
    for i, page in enumerate(doc):
        # 1. YÃ¶ntem: DoÄŸrudan metin Ã§Ä±karma
        txt = page.get_text("text")
        
        # EÄŸer metin Ã§ok azsa veya yoksa OCR dene (Fallback)
        if not txt or len(txt.strip()) < 10:
            print(f"Sayfa {i+1} metin iÃ§ermiyor, OCR deneniyor...")
            try:
                # SayfayÄ± gÃ¶rÃ¼ntÃ¼ye Ã§evir (300 DPI)
                pix = page.get_pixmap(dpi=300)
                ocr_txt = ocr_page(pix)
                if ocr_txt and ocr_txt.strip():
                    txt = ocr_txt
                    print(f"Sayfa {i+1} OCR ile okundu.")
            except Exception as e:
                print(f"Sayfa {i+1} OCR iÅŸlemi baÅŸarÄ±sÄ±z: {e}")
        
        if txt and txt.strip():
            parts.append(txt)
            
    return "\n".join(parts)

def _chunkify(text, size, overlap):
    out = []
    i, n = 0, len(text)
    while i < n:
        j = min(i + size, n)
        out.append(text[i:j])
        if j == n: break
        i = max(j - overlap, 0)
    return out

def index_pdf(file):
    if not file:
        return "Ã–nce bir PDF seÃ§in."
    text = _read_pdf(file.name)
    if not text.strip():
        return "PDFâ€™den metin Ã§Ä±karÄ±lamadÄ±."
    chunks = _chunkify(text, CHUNK_SIZE, CHUNK_OVERLAP)
    ids = [f"{os.path.basename(file.name)}::{i}" for i in range(len(chunks))]
    embs = list(embedder.embed(chunks))
    col.add(
        ids=ids,
        documents=chunks,
        embeddings=embs,
        metadatas=[{"source": os.path.basename(file.name), "chunk": i} for i in range(len(chunks))]
    )
    return f"Ä°ndeksleme tamam âœ…  {os.path.basename(file.name)} â†’ {len(chunks)} parÃ§a eklendi."

def clear_index():
    global col
    db.delete_collection(COLLECTION_NAME)
    col = db.create_collection(COLLECTION_NAME, metadata={"hnsw:space": "cosine"})
    return "Ä°ndeks temizlendi."

SYSTEM_RAG = (
    "TÃ¼rkÃ§e cevap ver. Sadece verilen baÄŸlamÄ± kullan; baÄŸlamda yoksa 'BaÄŸlamda yok' de. "
    "Kaynaklardan emin olmadÄ±ÄŸÄ±n bilgileri uydurma."
)

def retrieve(question):
    if not question.strip():
        return []
    qv = list(embedder.embed([question]))[0]
    res = col.query(query_embeddings=[qv], n_results=TOP_K, include=["documents","metadatas"])
    ctxs = []
    if res and res.get("documents") and res["documents"][0]:
        for d, m in zip(res["documents"][0], res["metadatas"][0]):
            src = m.get("source","?")
            ch = m.get("chunk","?")
            ctxs.append(f"[Kaynak: {src} | ParÃ§a: {ch}]\n{d}")
    return ctxs

def ask(question, history):
    ctxs = retrieve(question)
    ctx_text = "\n\n---\n\n".join(ctxs) if ctxs else "(baÄŸlam bulunamadÄ±)"

    msgs = [
        {"role":"system","content": SYSTEM_RAG},
        {"role":"user","content": f"BAÄžLAM:\n{ctx_text}\n\nSORU:\n{question}"}
    ]

    # AkÄ±ÅŸlÄ± yanÄ±t
    resp = client.chat.completions.create(
        model="local",
        messages=msgs,
        temperature=0.2,
        max_tokens=MAX_ANSWER_TOKENS,
        stream=True,
    )
    partial = ""
    for chunk in resp:
        delta = chunk.choices[0].delta.content or ""
        partial += delta
        yield partial

def summarize_last(n_sentences):
    try:
        # ChromaDB'den tÃ¼m dokÃ¼manlarÄ± al
        res = col.get(include=["documents","metadatas"])
        
        # documents yapÄ±sÄ±nÄ± kontrol et ve dÃ¼zleÅŸtir
        docs = res.get("documents") or []
        if not docs:
            return "Ã–zetlenecek baÄŸlam bulunamadÄ±."
        
        # EÄŸer iÃ§ iÃ§e liste varsa dÃ¼zleÅŸtir, deÄŸilse direkt kullan
        if docs and isinstance(docs[0], list):
            flat = docs[0]
        else:
            flat = docs if isinstance(docs, list) else list(docs)
        
        if not flat:
            return "Ã–zetlenecek baÄŸlam bulunamadÄ±."
        
        # n_sentences'Ä± integer'a Ã§evir (Slider float dÃ¶ndÃ¼rebilir)
        n_sent = int(n_sentences)
        
        # Ä°lk TOP_K parÃ§ayÄ± al
        sample = "\n\n---\n\n".join(flat[:TOP_K])
        
        msgs = [
            {"role":"system","content":"TÃ¼rkÃ§e, kÄ±sa ve madde madde Ã¶zet Ã§Ä±kar."},
            {"role":"user","content": f"AÅŸaÄŸÄ±daki metni {n_sent} maddeyle KISA Ã¶zetle:\n\n{sample}"}
        ]
        
        r = client.chat.completions.create(
            model="local", 
            messages=msgs, 
            temperature=0.2, 
            max_tokens=MAX_ANSWER_TOKENS,
            stream=False
        )
        
        return r.choices[0].message.content or "Ã–zet oluÅŸturulamadÄ±."
    
    except Exception as e:
        return f"Hata: {str(e)}"

with gr.Blocks(title="DoÄŸal Dil TabanlÄ± DokÃ¼man Analiz Sistemi") as demo:
    gr.Markdown("## DoÄŸal Dil TabanlÄ± DokÃ¼man Analiz Sistemi\nPDF yÃ¼kle, indeksle ve soru sor. BÃ¼yÃ¼k PDFâ€™lerde context aÅŸÄ±mÄ± olmaz.")

    with gr.Row():
        f = gr.File(label="PDF yÃ¼kle (.pdf)")
    with gr.Row():
        idx_btn = gr.Button("ðŸ“¥ PDFâ€™yi Ä°ndeksle")
        clr_btn = gr.Button("ðŸ§¹ Ä°ndeksi Temizle")
    log = gr.Textbox(label="Durum / Log", interactive=False)

    idx_btn.click(fn=index_pdf, inputs=f, outputs=log)
    clr_btn.click(fn=clear_index, inputs=None, outputs=log)

    gr.Markdown("### Soru-Cevap")
    chat = gr.ChatInterface(
        fn=ask,
        textbox=gr.Textbox(placeholder="Ã–rn: Bu belgede Ã¶nerilen yÃ¶ntem nedir?", container=False),
        autofocus=True
    )

    with gr.Row():
        n_sent = gr.Slider(3, 10, value=5, step=1, label="Ã–zet madde sayÄ±sÄ±")
        sum_btn = gr.Button("ðŸ“ KÄ±sa Ã–zet Al")
    summary_out = gr.Textbox(label="Ã–zet", lines=8)
    sum_btn.click(fn=summarize_last, inputs=n_sent, outputs=summary_out)

demo.launch(server_name="127.0.0.1", server_port=7861)
