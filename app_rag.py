import os, gradio as gr, fitz, re, time, csv, datetime
from openai import OpenAI
import pytesseract
from PIL import Image, ImageEnhance

# ---- Ayarlar ve Yollar ----
pytesseract.pytesseract.tesseract_cmd = os.getenv("TESSERACT_CMD", r"C:\Program Files\Tesseract-OCR\tesseract.exe")

# ---- LLM: yerel llama-server (OpenAI uyumlu) ----
llm_api_url = os.getenv("LLM_API_URL", "http://127.0.0.1:8080/v1")
client = OpenAI(api_key="local", base_url=llm_api_url)

# ---- VektÃ¶r veritabanÄ± & embedding ----
import chromadb
from chromadb.config import Settings
from fastembed import TextEmbedding

# FastEmbed desteklenen model (TÃ¼rkÃ§e iÃ§in de Ã§alÄ±ÅŸÄ±r)
EMBED_MODEL = "BAAI/bge-small-en-v1.5"  # âœ… FastEmbed tarafÄ±ndan destekleniyor
# Alternatifler:
# "sentence-transformers/all-MiniLM-L6-v2" - Daha hÄ±zlÄ±
# "BAAI/bge-base-en-v1.5" - Daha gÃ¼Ã§lÃ¼ ama yavaÅŸ

# Optimize edilmiÅŸ parametreler - ğŸ”¥ OCR iÃ§in GEVÅEK ayarlar
CHUNK_SIZE = 600  # Daha kÃ¼Ã§Ã¼k = daha spesifik eÅŸleÅŸme
CHUNK_OVERLAP = 250  # ğŸ”¥ ArtÄ±rÄ±ldÄ±: Daha fazla overlap = daha iyi baÄŸlam
TOP_K = 8  # DAHA FAZLA kaynak = daha iyi ÅŸans
MAX_ANSWER_TOKENS = 2000
SIMILARITY_THRESHOLD = 0.35  # ğŸ”¥ DÃœÅÃœRÃœLDÃœ: OCR hatalarÄ±na toleranslÄ± (0.40 â†’ 0.35)

PERSIST_DIR = "./rag_store"
COLLECTION_NAME = "pdf_chunks"

embedder = TextEmbedding(model_name=EMBED_MODEL)

db = chromadb.PersistentClient(path=PERSIST_DIR, settings=Settings(allow_reset=True))
try:
    col = db.get_collection(COLLECTION_NAME)
except:
    col = db.create_collection(COLLECTION_NAME, metadata={"hnsw:space": "cosine"})

# ---- GELÄ°ÅTÄ°RÄ°LMÄ°Å SYSTEM PROMPT ----
SYSTEM_RAG = """Sen bir dokÃ¼man analiz asistanÄ±sÄ±n.

GÃ–REV: Verilen BAÄLAM'Ä± kullanarak soruyu cevapla.

KURALLAR:
- BAÄLAM'da cevap varsa â†’ DokÃ¼mandaki bilgiyi kullanarak cevapla
- BAÄLAM'da cevap yoksa â†’ "Bu bilgi dokÃ¼manda bulunmuyor" de
- CevaplarÄ±nÄ± TÃ¼rkÃ§e, net ve anlaÅŸÄ±lÄ±r yaz

Ã–NEMLÄ°: Sadece verilen BAÄLAM'Ä± kullan. Kendi genel bilgini ekleme."""

def ocr_page(page_pixmap):
    """GeliÅŸtirilmiÅŸ OCR - contrast ve denoise ile"""
    try:
        img = Image.frombytes("RGB", [page_pixmap.width, page_pixmap.height], page_pixmap.samples)
        
        # GÃ¶rÃ¼ntÃ¼yÃ¼ bÃ¼yÃ¼t (daha iyi OCR iÃ§in)
        img = img.resize((img.width * 2, img.height * 2), Image.Resampling.LANCZOS)
        
        # Griye Ã§evir
        img = img.convert('L')
        
        # Kontrast artÄ±r (OCR iÃ§in daha iyi)
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2.5)
        
        # ParlaklÄ±k artÄ±r
        enhancer = ImageEnhance.Brightness(img)
        img = enhancer.enhance(1.2)
        
        # Keskinlik artÄ±r
        enhancer = ImageEnhance.Sharpness(img)
        img = enhancer.enhance(2.0)
        
        # OCR config - TÃ¼rkÃ§e iÃ§in optimize (PSM 3 = tam sayfa)
        custom_config = r'--oem 3 --psm 3 -l tur'
        text = pytesseract.image_to_string(img, config=custom_config)
        
        return text
    except Exception as e:
        print(f"OCR HatasÄ±: {e}")
        return ""

def _read_pdf(path):
    """PDF okuma - metin + OCR fallback"""
    doc = fitz.open(path)
    parts = []
    
    print(f"ğŸ“„ PDF Analiz ediliyor: {path}")
    print(f"   Toplam sayfa sayÄ±sÄ±: {len(doc)}")
    
    for i, page in enumerate(doc):
        txt = page.get_text("text")
        
        # Metin varsa kontrol et - ama Ã§ok az veya garip karakterler varsa OCR dene
        text_length = len(txt.strip())
        
        print(f"\n   ğŸ“„ Sayfa {i+1}:")
        print(f"      Metin Ã§Ä±karma: {text_length} karakter")
        
        # EÄŸer metin Ã§ok azsa veya %80'den fazlasÄ± sayÄ±/Ã¶zel karakter ise OCR dene
        should_ocr = False
        if text_length < 50:
            should_ocr = True
            print(f"      âš ï¸ Metin Ã§ok az, OCR gerekli")
        elif text_length < 200:
            # Metin var ama az - belki dÃ¼zgÃ¼n deÄŸil, OCR'Ä± da dene
            alphanum = sum(c.isalnum() for c in txt)
            if alphanum / len(txt) < 0.5:  # %50'den az alfanumerik karakter
                should_ocr = True
                print(f"      âš ï¸ Metin kalitesiz (%{int(alphanum/len(txt)*100)} alfanumerik), OCR gerekli")
        
        if should_ocr:
            print(f"      ğŸ”„ OCR iÅŸlemi baÅŸlÄ±yor (DPI: 400)...")
            try:
                # YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼k (DPI 400)
                pix = page.get_pixmap(dpi=400)
                ocr_txt = ocr_page(pix)
                ocr_length = len(ocr_txt.strip())
                
                print(f"      ğŸ“ OCR sonucu: {ocr_length} karakter")
                
                if ocr_txt and ocr_length > text_length:
                    txt = ocr_txt
                    print(f"      âœ… OCR kullanÄ±ldÄ± ({ocr_length} > {text_length})")
                    
                    # Ä°lk 200 karakteri gÃ¶ster (debug iÃ§in)
                    preview = ocr_txt[:200].replace('\n', ' ')
                    print(f"      ğŸ‘ï¸ Ã–nizleme: {preview}...")
                else:
                    print(f"      âš ï¸ OCR sonucu kÃ¶tÃ¼, orijinal metin kullanÄ±lÄ±yor")
            except Exception as e:
                print(f"      âŒ OCR hatasÄ±: {e}")
        else:
            print(f"      âœ… Metin yeterli ({text_length} karakter)")
            # Ä°lk 200 karakteri gÃ¶ster
            preview = txt[:200].replace('\n', ' ')
            print(f"      ğŸ‘ï¸ Ã–nizleme: {preview}...")
        
        if txt and txt.strip():
            parts.append(txt)
        else:
            print(f"      âš ï¸ Bu sayfa boÅŸ, atlanÄ±yor")
            
    full_text = "\n".join(parts)
    print(f"\nâœ… TOPLAM: {len(full_text)} karakter, {len(parts)} sayfa iÅŸlendi")
    
    if len(full_text) < 100:
        print("âš ï¸ UYARI: Ã‡ok az metin Ã§Ä±karÄ±ldÄ±! OCR ayarlarÄ±nÄ± kontrol edin.")
    
    return full_text

def _chunkify_smart(text, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """AkÄ±llÄ± parÃ§alama - cÃ¼mle sÄ±nÄ±rlarÄ±na gÃ¶re"""
    # CÃ¼mlelere bÃ¶l (TÃ¼rkÃ§e noktalama)
    sentences = re.split(r'(?<=[.!?;:])\s+', text)
    
    chunks = []
    current_chunk = ""
    
    for sent in sentences:
        # BoÅŸ cÃ¼mleleri atla
        if not sent.strip():
            continue
            
        # EÄŸer bu cÃ¼mleyi ekleyince boyut aÅŸÄ±lÄ±rsa
        if len(current_chunk) + len(sent) + 1 > size:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sent + " "
        else:
            current_chunk += sent + " "
    
    # Son chunk'Ä± ekle
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    # Overlap ekle (son N kelimeyi bir sonraki chunk'a taÅŸÄ±)
    if overlap > 0 and len(chunks) > 1:
        overlapped = [chunks[0]]
        for i in range(1, len(chunks)):
            prev_words = chunks[i-1].split()
            overlap_words = prev_words[-min(overlap//5, len(prev_words)):]  # Kelime bazlÄ± overlap
            overlapped.append(" ".join(overlap_words) + " " + chunks[i])
        return overlapped
    
    return chunks

def index_pdf(file):
    """PDF'yi indeksle - geliÅŸtirilmiÅŸ feedback"""
    if not file:
        return "âš ï¸ Ã–nce bir PDF dosyasÄ± seÃ§in."
    
    try:
        text = _read_pdf(file.name)
        
        if not text.strip():
            return "âŒ PDF'den metin Ã§Ä±karÄ±lamadÄ±. OCR desteÄŸi aktif mi kontrol edin."
        
        # AkÄ±llÄ± chunking
        chunks = _chunkify_smart(text, CHUNK_SIZE, CHUNK_OVERLAP)
        
        if not chunks:
            return "âŒ Metin parÃ§alanamadÄ±."
        
        # Embedding oluÅŸtur
        print(f"ğŸ”„ {len(chunks)} parÃ§a iÃ§in embedding oluÅŸturuluyor...")
        ids = [f"{os.path.basename(file.name)}::{i}" for i in range(len(chunks))]
        embs = list(embedder.embed(chunks))
        
        # Zaman damgasÄ± ekle
        current_time = time.time()
        
        # ChromaDB'ye ekle
        col.add(
            ids=ids,
            documents=chunks,
            embeddings=embs,
            metadatas=[{
                "source": os.path.basename(file.name), 
                "chunk": i,
                "length": len(chunks[i]),
                "timestamp": current_time
            } for i in range(len(chunks))]
        )
        
        # Ä°statistikler
        avg_len = sum(len(c) for c in chunks) / len(chunks)
        return f"""âœ… Ä°ndeksleme BaÅŸarÄ±lÄ±!
        
ğŸ“Š Ä°statistikler:
â€¢ Dosya: {os.path.basename(file.name)}
â€¢ Toplam Metin: {len(text):,} karakter
â€¢ ParÃ§a SayÄ±sÄ±: {len(chunks)}
â€¢ Ortalama ParÃ§a Boyutu: {int(avg_len)} karakter
â€¢ Embedding Modeli: {EMBED_MODEL}

ArtÄ±k soru sorabilirsiniz! ğŸ’¬"""
        
    except Exception as e:
        return f"âŒ Hata oluÅŸtu: {str(e)}"

def clear_index():
    """Son yÃ¼klenen PDF'yi sil (Zaman damgasÄ±na gÃ¶re en son)"""
    global col
    try:
        all_data = col.get(include=["metadatas"])
        
        if not all_data or not all_data["metadatas"]:
            return "âš ï¸ Silinecek belge yok."
        
        # DosyalarÄ± ve zaman damgalarÄ±nÄ± topla
        file_timestamps = {}
        source_ids = {}
        
        for i, meta in enumerate(all_data["metadatas"]):
            source = meta.get("source", "unknown")
            timestamp = meta.get("timestamp", 0)
            
            # ID'leri kaydet
            if source not in source_ids:
                source_ids[source] = []
            source_ids[source].append(all_data["ids"][i])
            
            # En gÃ¼ncel timestamp'i bul
            if source not in file_timestamps:
                file_timestamps[source] = timestamp
            else:
                file_timestamps[source] = max(file_timestamps[source], timestamp)
        
        if not file_timestamps:
            return "âš ï¸ Silinecek belge yok."
        
        # En son eklenen dosyayÄ± bul
        last_source = max(file_timestamps, key=file_timestamps.get)
        ids_to_delete = source_ids[last_source]
        
        # Kalan dosya sayÄ±sÄ±
        remaining_count = len(file_timestamps) - 1
        
        # Silme iÅŸlemi
        col.delete(ids=ids_to_delete)
        
        return f"""ğŸ—‘ï¸ Son yÃ¼klenen PDF silindi!
        
ğŸ“Š Silinen:
â€¢ Dosya: {last_source}
â€¢ ParÃ§a sayÄ±sÄ±: {len(ids_to_delete)}

ğŸ“š Kalan dosya sayÄ±sÄ±: {remaining_count}

ğŸ’¡ TÃ¼m indeksi silmek iÃ§in "TÃ¼m Ä°ndeksi Sil" butonunu kullanÄ±n."""
        
    except Exception as e:
        import traceback
        return f"âŒ Temizleme hatasÄ±: {str(e)}\n{traceback.format_exc()}"

def clear_all_index():
    """TÃœM indeksi temizle - db.reset() kullanarak"""
    global col
    try:
        # Reset ile veri tabanÄ±nÄ± sÄ±fÄ±rla
        db.reset()
        
        # Koleksiyonu tekrar oluÅŸtur
        col = db.get_or_create_collection(COLLECTION_NAME, metadata={"hnsw:space": "cosine"})
        
        return f"""ğŸ§¹ TÃœM Ä°NDEKS VE VERÄ°TABANI SIFIRLANDI!
        
âš ï¸ Bilgi:
"rag_store" klasÃ¶rÃ¼ diskte gÃ¶rÃ¼nmeye devam edebilir, Ã§Ã¼nkÃ¼ uygulama Ã§alÄ±ÅŸtÄ±ÄŸÄ± sÃ¼rece veritabanÄ± baÄŸlantÄ±sÄ± aktiftir. 
Ancak iÃ§i tamamen boÅŸtur ve tÃ¼m veriler silinmiÅŸtir.

âœ… VeritabanÄ± tertemiz!

ğŸ¯ Yeni PDF yÃ¼kleyebilirsiniz."""
        
    except Exception as e:
        import traceback
        return f"âŒ Temizleme hatasÄ±: {str(e)}\n{traceback.format_exc()}"

def log_to_csv(question, answer):
    """Soru ve cevabÄ± CSV dosyasÄ±na kaydeder"""
    try:
        if not os.path.exists(PERSIST_DIR):
            os.makedirs(PERSIST_DIR)
            
        log_file = os.path.join(PERSIST_DIR, "chat_history.csv")
        file_exists = os.path.exists(log_file)
        
        with open(log_file, mode='a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(["Zaman", "Soru", "Cevap"])
            
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            writer.writerow([timestamp, question, answer])
            print(f"ğŸ’¾ Sohbet kaydedildi: {timestamp}")
    except Exception as e:
        print(f"âŒ Log hatasÄ±: {e}")

def retrieve(question, top_k=TOP_K, show_scores=True):
    """GeliÅŸtirilmiÅŸ retrieval - skor bazlÄ± filtreleme"""
    if not question.strip():
        return []
    
    try:
        print(f"\nğŸ” RETRIEVAL: '{question}'")
        
        # Soru embedding
        qv = list(embedder.embed([question]))[0]
        print(f"   âœ… Soru embedding oluÅŸturuldu (boyut: {len(qv)})")
        
        # Daha fazla sonuÃ§ al, sonra filtrele
        res = col.query(
            query_embeddings=[qv], 
            n_results=min(top_k * 3, 20),  # 3x al, en iyileri seÃ§
            include=["documents", "metadatas", "distances"]
        )
        
        if not res or not res.get("documents") or not res["documents"][0]:
            print("   âš ï¸ HiÃ§ sonuÃ§ bulunamadÄ±!")
            return []
        
        print(f"   ğŸ“Š {len(res['documents'][0])} sonuÃ§ bulundu")
        
        ctxs = []
        for d, m, dist in zip(
            res["documents"][0], 
            res["metadatas"][0],
            res["distances"][0]
        ):
            # Cosine distance'Ä± similarity score'a Ã§evir (0-1)
            similarity = 1 - (dist / 2)
            
            print(f"      â€¢ Benzerlik: {similarity:.2%} | Kaynak: {m.get('source', '?')} #{m.get('chunk', '?')}")
            
            # Threshold kontrolÃ¼
            if similarity >= SIMILARITY_THRESHOLD:
                ctxs.append({
                    "text": d,
                    "source": m.get("source", "?"),
                    "chunk": m.get("chunk", "?"),
                    "score": similarity,
                    "length": m.get("length", len(d))
                })
                print(f"        âœ… EÅŸik geÃ§ildi (%{SIMILARITY_THRESHOLD*100:.0f})")
            else:
                print(f"        âŒ EÅŸik altÄ±nda (%{SIMILARITY_THRESHOLD*100:.0f})")
        
        # En iyi top_k'yÄ± seÃ§
        ctxs = sorted(ctxs, key=lambda x: x["score"], reverse=True)[:top_k]
        
        print(f"   âœ… {len(ctxs)} kaynak kullanÄ±lacak (TOP_K={top_k})")
        
        if ctxs and show_scores:
            print(f"\n   ğŸ“‹ SeÃ§ilen Kaynaklar:")
            for i, ctx in enumerate(ctxs, 1):
                preview = ctx['text'][:100].replace('\n', ' ')
                print(f"      {i}. [{ctx['score']:.2%}] {ctx['source']} #{ctx['chunk']}")
                print(f"         â†’ {preview}...")
        
        return ctxs
    
    except Exception as e:
        print(f"âŒ Retrieval hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return []

def ask(question, history):
    """Ana RAG fonksiyonu - akÄ±ÅŸlÄ± yanÄ±t"""
    if not question.strip():
        yield "âš ï¸ LÃ¼tfen bir soru yazÄ±n."
        return
    
    # Retrieval
    ctxs = retrieve(question, show_scores=True)
    
    if not ctxs:
        yield f"""âš ï¸ **Sorunuzla ilgili bilgi bulunamadÄ±.**

OlasÄ± nedenler:
â€¢ PDF henÃ¼z indekslenmemiÅŸ olabilir
â€¢ Sorunuz dokÃ¼mandaki iÃ§erikle eÅŸleÅŸmiyor
â€¢ Benzerlik eÅŸiÄŸi Ã§ok yÃ¼ksek (ÅŸu an: {SIMILARITY_THRESHOLD:.0%})

ğŸ’¡ **Ã–neriler:**
â€¢ Soruyu farklÄ± kelimelerle tekrar deneyin
â€¢ PDF'nin doÄŸru yÃ¼klendiÄŸinden emin olun
â€¢ Daha genel sorular sorun"""
        return
    
    # Context'i formatla
    ctx_parts = []
    for i, ctx in enumerate(ctxs, 1):
        ctx_parts.append(
            f"[KAYNAK {i}: {ctx['source']} | ParÃ§a #{ctx['chunk']} | "
            f"Ä°lgililik: {ctx['score']:.1%}]\n\n{ctx['text']}"
        )
    ctx_text = "\n\n{'='*60}\n\n".join(ctx_parts)
    
    # Basit ve net prompt
    user_prompt = f"""AÅŸaÄŸÄ±daki BAÄLAM bilgilerini kullanarak soruyu cevapla.

BAÄLAM:
{ctx_text}

SORU: {question}

CEVAP (TÃ¼rkÃ§e, baÄŸlama gÃ¶re):"""

    msgs = [
        {"role": "system", "content": SYSTEM_RAG},
        {"role": "user", "content": user_prompt}
    ]

    try:
        # ğŸ”¥ YENÄ°: Tekrar Ã¶nleme parametreleri
        resp = client.chat.completions.create(
            model="local",
            messages=msgs,
            temperature=0.1,
            max_tokens=MAX_ANSWER_TOKENS,
            top_p=0.9,
            frequency_penalty=0.0,
            presence_penalty=0.0,
            stop=["BAÄLAM:", "SORU:", "---"],  # ğŸ”¥ DÃ¶ngÃ¼ kÄ±rÄ±cÄ±lar
            stream=True,
        )
        
        partial = ""
        
        for chunk in resp:
            delta = chunk.choices[0].delta.content or ""
            partial += delta
            
            yield partial
            
        # YanÄ±t tamamlanÄ±nca kaydet
        log_to_csv(question, partial)
            
    except Exception as e:
        yield f"âŒ LLM hatasÄ±: {str(e)}"

def ask_with_debug(question, history):
    """Debug modlu soru sorma - kaynak bilgilerini gÃ¶ster"""
    if not question.strip():
        yield "âš ï¸ LÃ¼tfen bir soru yazÄ±n."
        return
    
    ctxs = retrieve(question, show_scores=False)
    
    if not ctxs:
        yield "âš ï¸ Ä°lgili kaynak bulunamadÄ±!"
        return
    
    # Debug bilgisi
    debug_info = "### ğŸ” Bulunan Kaynaklar:\n\n"
    for i, ctx in enumerate(ctxs, 1):
        debug_info += f"**{i}. {ctx['source']}** (ParÃ§a #{ctx['chunk']})\n"
        debug_info += f"   â€¢ Ä°lgililik Skoru: **{ctx['score']:.1%}**\n"
        debug_info += f"   â€¢ Uzunluk: {ctx['length']} karakter\n"
        debug_info += f"   â€¢ Ã–nizleme: _{ctx['text'][:120].strip()}..._\n\n"
    
    debug_info += "\n---\n\n### ğŸ’¬ Model CevabÄ±:\n\n"
    yield debug_info
    
    # Normal yanÄ±t akÄ±ÅŸÄ±
    ctx_text = "\n\n---\n\n".join([c["text"] for c in ctxs])
    
    user_prompt = f"""BAÄLAM:\n{ctx_text}\n\nSORU: {question}\n\nCEVAP (TÃ¼rkÃ§e, sadece baÄŸlama gÃ¶re):"""
    
    msgs = [
        {"role": "system", "content": SYSTEM_RAG},
        {"role": "user", "content": user_prompt}
    ]
    
    try:
        resp = client.chat.completions.create(
            model="local",
            messages=msgs,
            temperature=0.1,
            max_tokens=MAX_ANSWER_TOKENS,
            frequency_penalty=0.0,
            presence_penalty=0.0,
            stop=["BAÄLAM:", "---"],
            stream=True,
        )
        
        full_answer = ""
        for chunk in resp:
            delta = chunk.choices[0].delta.content or ""
            debug_info += delta
            full_answer += delta
            
            yield debug_info
            
        # YanÄ±t tamamlanÄ±nca kaydet
        log_to_csv(question, full_answer)
            
    except Exception as e:
        yield debug_info + f"\n\nâŒ Hata: {str(e)}"

def summarize_doc():
    """Belgeyi genel olarak Ã¶zetle"""
    try:
        res = col.get(include=["documents", "metadatas"])
        docs = res.get("documents") or []
        
        if not docs:
            return "âš ï¸ Ã–zetlenecek belge bulunamadÄ±. Ã–nce bir PDF indeksleyin."
        
        # TÃ¼m chunk'larÄ± birleÅŸtir
        all_text = " ".join(docs) if isinstance(docs, list) else str(docs)
        
        # Ã‡ok uzunsa ilk N chunk'Ä± al (limitli context)
        if len(all_text) > 15000:
            sample_chunks = docs[:10] if isinstance(docs, list) else [all_text[:15000]]
            sample_text = " ".join(sample_chunks) if isinstance(sample_chunks, list) else sample_chunks
        else:
            sample_text = all_text
        
        msgs = [
            {"role": "system", "content": "Sen bir uzman dokÃ¼man analistisin. Verilen metnin genel ve kapsamlÄ± bir Ã¶zetini Ã§Ä±kar."},
            {"role": "user", "content": f"""AÅŸaÄŸÄ±daki belgenin iÃ§eriÄŸini, ana konusunu ve Ã¶nemli noktalarÄ±nÄ± kapsayan genel bir Ã¶zet yaz.
AnlaÅŸÄ±lÄ±r paragraflar halinde, akÄ±cÄ± bir TÃ¼rkÃ§e kullan.

BELGENÄ°N METNÄ° (KÄ±smi):
{sample_text}

GENEL Ã–ZET:"""}
        ]
        
        r = client.chat.completions.create(
            model="local", 
            messages=msgs, 
            temperature=0.2, 
            max_tokens=MAX_ANSWER_TOKENS,
            stream=False
        )
        
        summary = r.choices[0].message.content or "Ã–zet oluÅŸturulamadÄ±."
        
        return f"### ğŸ“ Belge Ã–zeti\n\n{summary}\n\n---\n_Not: Bu Ã¶zet belgenin ilk {len(sample_text)} karakterine dayanmaktadÄ±r._"
    
    except Exception as e:
        return f"âŒ Ã–zet hatasÄ±: {str(e)}"

# ---- GRADIO ARAYÃœZÃœ ----
with gr.Blocks(title="DOÄAL DÄ°L TABANLI DOKÃœMAN ANALÄ°Z SÄ°STEMÄ°") as demo:
    gr.Markdown("""
    # ğŸ¯ DOÄAL DÄ°L TABANLI DOKÃœMAN ANALÄ°Z SÄ°STEMÄ°
    
    ### Ã–zellikler:
    - ğŸ“„ PDF metin Ã§Ä±karma + OCR desteÄŸi
    - ğŸ§  AkÄ±llÄ± vektÃ¶r tabanlÄ± arama
    - ğŸ’¬ BaÄŸlama dayalÄ± soru-cevap
    - ğŸ” Debug modu ile kaynak gÃ¶rÃ¼ntÃ¼leme
    
    **Model:** BAAI/bge-small-en-v1.5 | **Chunk:** 800 karakter | **Overlap:** 200
    """)

    with gr.Row():
        f = gr.File(label="ğŸ“ PDF DosyasÄ± YÃ¼kle (.pdf)", file_types=[".pdf"])
    
    with gr.Row():
        idx_btn = gr.Button("ğŸ“¥ PDF'yi Ä°ndeksle", variant="primary", size="lg")
        clr_btn = gr.Button("ğŸ—‘ï¸ Son PDF'i Sil", variant="secondary")
        clr_all_btn = gr.Button("ğŸ§¹ TÃ¼m Ä°ndeksi Sil", variant="stop")
    
    log = gr.Textbox(label="ğŸ“Š Durum / Ä°statistikler", interactive=False, lines=8)

    idx_btn.click(fn=index_pdf, inputs=f, outputs=log)
    clr_btn.click(fn=clear_index, inputs=None, outputs=log)
    clr_all_btn.click(fn=clear_all_index, inputs=None, outputs=log)

    gr.Markdown("---")
    gr.Markdown("## ğŸ’¬ Soru-Cevap")
    
    with gr.Tab("Normal Mod"):
        chat_normal = gr.ChatInterface(
            fn=ask,
            textbox=gr.Textbox(
                placeholder="Ã–rn: Bu belgede deyimler nasÄ±l tanÄ±mlanÄ±yor?", 
                container=False,
                scale=7
            )
        )
    
    with gr.Tab("ğŸ” Debug Mod (Kaynaklarla)"):
        chat_debug = gr.ChatInterface(
            fn=ask_with_debug,
            textbox=gr.Textbox(
                placeholder="Debug modunda kaynak bilgileri de gÃ¶sterilir", 
                container=False
            )
        )

    gr.Markdown("---")
    gr.Markdown("## ğŸ“ Belge Ã–zeti")
    
    with gr.Row():
        sum_btn = gr.Button("ğŸ“ Genel Ã–zet OluÅŸtur", variant="primary")
    
    summary_out = gr.Textbox(label="Ã–zet Sonucu", lines=10)
    sum_btn.click(fn=summarize_doc, inputs=None, outputs=summary_out)
    
    gr.Markdown("""
    ---
    ### ğŸ’¡ KullanÄ±m Ä°puÃ§larÄ±:
    - SorularÄ±nÄ±zÄ± net ve spesifik sorun ("Bu belgede X nedir?")
    - Debug modunu kullanarak hangi kaynaklarÄ±n bulunduÄŸunu gÃ¶rebilirsiniz
    - Benzerlik eÅŸiÄŸi: %65 (koddan deÄŸiÅŸtirilebilir)
    - PDF'yi indekslemeden Ã¶nce soru sormayÄ±n
    """)

server_name = os.getenv("GRADIO_SERVER_NAME", "127.0.0.1")
demo.launch(server_name=server_name, server_port=7861, share=False)